{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas - partie 2\n",
    "\n",
    "<img src=\"https://pandas.pydata.org/docs/_static/pandas.svg\" alt=\"logo pandas\" width=\"500\"/>\n",
    "\n",
    "Dans la première partie, nous avons vous l'objet qui se trouve au coeur de la librairie pandas, cousine de numpy. Pour rappel, un dataset (ou jeu de données) et la donnée brute, on la transforme en DataFrame, objet qui ressemble à un tableau excel où chaque colonne possède un nom. Ainsi, il est dit que les données d'un DataFrame sont tabulaires.\n",
    "\n",
    "La partie 1 du TP pandas a été l'occasion de travailler avec de petits dataframes (écrits à la main), toutefois en Big Data, ce n'est pas des DataFrame de moins de 60 lignes avec lesquels nous travaillons, mais en général plus de 100 000. Bien évidemment, nous n'allons pas écrire à la main, ces gros fichiers. C'est ici qu'entre en jeu l'Open Data, notamment popularisé par le \"Florida Man\". L'Open Data ou Données Ouvertes permettent à tous de récupérer des données exposées par des gouvernements et autres institutions pour les exploiter. Ces données peuvent être sous plusieurs formes mais celle la plus courante est le format csv.\n",
    "\n",
    "Et ces données, on les trouve où ? Il existe beaucoup de sources, dans le cadre de cette pratique, nous allons utiliser celui des naissances de 1900 à l'année en cours par département (voir lien plus bas). Mais voici quelques sites :\n",
    "\n",
    "- [Voir site des données ouvertes du gouvernement](https://www.data.gouv.fr/)\n",
    "- [Voir site des données ouvertes de l'insee](https://www.insee.fr/fr/statistiques?categorie=1)\n",
    "- [Voir moteur de recherche de Google dédié aux datasets](https://datasetsearch.research.google.com/)\n",
    "- [Voir site des données ouvertes de l'éducation nationale](https://data.education.gouv.fr/)\n",
    "- [Voir site de données ouvertes (souvent anglophones)](https://github.com/awesomedata/awesome-public-datasets)\n",
    "\n",
    "---\n",
    "- [Voir source du fichier de données des prénoms - Fichier que nous allons utiliser pour la suite du TP](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262#consulter)\n",
    "  - Pas besoin de télécharger l'archive contenant le csv par département depuis ce lien, un lien direct a été mis plus bas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comma-separated values ou csv est un format de fichier représentant des valeurs tabulaires sous forme de valeurs séparées par des virgules, ce caractère étant celui de séparation par défaut. Chaque valeur entre virgule représentant une cellule.\n",
    "\n",
    "![csv](_images/csv.jpg)\n",
    "- [En savoir plus sur le format csv (wikipedia)](https://fr.wikipedia.org/wiki/Comma-separated_values)\n",
    "\n",
    "Il est possible de transformer un fichier tableur (.xls, .xslx, .ods...) en fichier csv, il suffit juste à la sauvegarde de préciser le format csv. \n",
    "\n",
    "![sauvegarde-csv](_images/sauvegarde-csv.jpg)\n",
    "\n",
    "Néanmoins pandas est très polyvalent, il est possible d'importer un fichier excel (xls/xslx), un fichier sql ou encore un tableau HTML en provenance d'un site. Pour les fichiers excel (xls/xslx), il faudra préciser le classeur que l'on souhaite charger.\n",
    "\n",
    "- [Voir liste des types gérés par pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cadre de ce cours, nous allons principalement manipuler des fichiers csv et les charger grâce à la méthode `pd.read_csv(\"chemin-du-fichier.csv\")` de pandas. Il faudra importer pandas (`import pandas as pd`) avant d'appeler la méthode `read_csv()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour les utilisateurs de Google colab\n",
    "\n",
    "Petit apparté pour les utilisateurs de google colab. Pour charger un fichier local, il faudra rajouter les lignes de codes suivantes :\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "# Très important : le nom du fichier passé en paramètre de la fonction \"uploaded\" doit avoir le même nom que le fichier que vous avez uploadé sinon, \n",
    "# vous aurez forcément une erreur\n",
    "df = pd.read_csv(io.BytesIO(uploaded['nom-du-fichier-uploader.csv']))\n",
    "```\n",
    "- **Ces lignes doivent être avant la manipulation d'un DataFrame et de préférence dans une cellule dédiée pour éviter d'uploader votre fichier à chaque fois**\n",
    "- **Vous ne pouvez pas importer de fichiers en navigation privée**\n",
    "\n",
    "- [Voir plus d'informations sur le chargement de fichiers externes avec Google colab - anglais](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement d'un fichier distant\n",
    "\n",
    "Bien que nous fassions du Python depuis Jupyter, nous avons toujours accès aux méthodes et classes natives de Python dont \"request\". Elle nous permet d'effectuer des rêquetes serveurs et donc de charger des fichiers\n",
    "\n",
    "```python\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "\n",
    "request.urlretrieve(\"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/decouvrez-les-librairies-python-pour-la-data-science/hubble_data.csv\", \"hubble.csv\")\n",
    "hubble = pd.read_csv(\"hubble.csv\")\n",
    "```\n",
    "\n",
    "Le code ci-dessus est là à titre indicatif, nous n'aurons pas besoin d'utiliser des fichiers distants dans nos pratiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0 : Téléchargement du dataset\n",
    "- [Télécharger dataset des prénoms](https://github.com/DanYellow/cours/blob/main/big-data-s4/datasets/naissances-par-departement-1900-2020.zip) (Cliquez sur le bouton \"Télécharger / Download\" sur github)\n",
    "  - Décompressez l'archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Petite note** : Le fichier possède plus de 3 500 000 lignes. Si on essaye d'ouvrir le fichier avec OpenOffice Calc (équivalent Open Source d'Excel), on a le droit au message suivant :\n",
    "\n",
    "![erreur chargement](_images/erreur-chargement-calc.jpg)\n",
    "\n",
    "Heureusement, nous utilisons pandas nous allons pouvoir ouvrir le fichier sans encombres.\n",
    "\n",
    "# Phase 1 : Récupération / Chargement du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisateurs de Google Colab\n",
    "# pensez bien à uploader le fichier dans cette cellule\n",
    "# avec le code plus haut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On importe pandas, pas de pandas, pas de DataFrame donc pas de manipulation de données tabulaires\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "# On charge le fichier dans une cellule spécifique. Pourquoi ? On veut éviter de charger ce gros fichier régulièrement.\n",
    "# Si nous utilisez Google Colab, ça devrait aller plus vite.\n",
    "# Le premier paramètre de notre fonction est très important, c'est le chemin vers votre fichier, comme en HTML si vous \n",
    "# changez le nom du fichier, il faudra penser à éditer le chemin dans la méthode.\n",
    "liste_prenoms_source = pd.read_csv(\"A-REMPLACER\", sep=\";\") # le fichier est chargé en tant que DataFrame\n",
    "\n",
    "# La fonction \"read_csv\" possède plein de paramètres permettant la lecture d'un fichier csv beaucoup plus souple, on peut, par exemple, importer certaines colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note importante sur les fichiers CSV\n",
    "\n",
    "Le csv que nous allons utiliser utilise des point-virgules ( ; ) pour séparer les colonnes d'où le paramètre `sep=\";\"` dans le code, il indique le séparateur des colonnes permettant ainsi d'afficher, de manipuler correctement notre DataFrame. Si le séparateur est incorrect, une erreur peut être levée ou le DataFrame sera consititué que d'une seule colonne (voir image ci-dessous).\n",
    "\n",
    "![mauvais séparateur de colonnes](_images/dataframe-mauvais-separateur.png)\n",
    "\n",
    "Pensez à changer le caractère de séparation des cellules de votre csv en cas de problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 : Exploration des données\n",
    "Après avoir chargé un DataFrame, la première chose à faire est de l'explorer. Cette phase nous permet de savoir assez facilement et rapidement quel jeu de données on manipule, on essaye de répondre aux questions suivantes pour mieux comprendre notre DataFrame :\n",
    "- A quoi ressemble notre DataFrame ? # `df.head()` ou `df.tail()` ou `display(df)` / `display(df)` équivaut à faire `df.head()` et `df.tail()` en même temps\n",
    "  - Ces méthodes vont afficher les 5 premières/dernières lignes, il est possible de passer un nombre en paramètre pour en afficher plus ou moins\n",
    "- Combien de lignes et colonnes possède-t-il ? # `df.shape`\n",
    "- Quel est le type de chaque colonne ? # `df.dtypes`\n",
    "- Quels sont les noms des colonnes ? # `df.columns.values.tolist()`\n",
    "- Existe-il des données absentes ou nulles ? / Combien il y a de données absentes ou nulles ? # `df.isnull().any()` ou `df.isna().any()` / `df.isnull().sum()` ou `df.isna().sum()`\n",
    "- Quelles données statistiques ressortent de mon jeu de données ? # `df.describe()`\n",
    "  - Note : Vu que les données n'ont pas encore été nettoyées (Phase 3), on peut obtenir des résultats étranges\n",
    "- Combien de valeurs uniques existe-t-il ? # `df.value_counts()`\n",
    "- Quelle est valeur min/max de certaines colonnes ? # `df['colonne'].min()/.max()`\n",
    "- Combien d'espace mémoire consomme mon DataFrame ? # `df.info()`\n",
    "- Combien de valeurs uniques existe-t-il par colonne ? / Quelles sont ces valeurs uniques ? # `df.nunique()` / `df[\"nom_du_champ\"].unique()`\n",
    "  - Attention concernant `df[\"nom_du_champ\"].unique()`, si vous avez beaucoup de valeurs uniques jupyter risque de ne pas tout afficher\n",
    "  - Écrire `df[\"nom_du_champ\"].unique()` pour chaque champ peut être fastideux, ainsi il est possible d'écrire à la place `pd.Series({colonne: df[colonne].unique() for colonne in df})`. Ceci affichera toutes les valeurs uniques pour chaque champs\n",
    "\n",
    "`.values.tolist()` a été rajouté pour s'assurer que toutes les lignes soient affichées dans Jupyter, l'outil ayant tendance à tronquer l'affichage quand il y a trop de résultats.\n",
    "\n",
    "Il n'est pas utile de répondre à toutes ces questions, toutefois, il faut au moins répondre aux questions suivantes : \n",
    "- A quoi ressemble notre DataFrame ?\n",
    "- Quels sont les noms des colonnes ?\n",
    "- Combien de lignes/colonnes possède-t-il ?\n",
    "- Existe-il des données absentes / nulles ?\n",
    "\n",
    "Ces quatre questions nous aideront beaucoup pour la phase suivante, mais répondre à plus de questions pourra encore plus nous aider.\n",
    "\n",
    "## A vous de coder\n",
    "Répondre en code aux questions posées plus haut, inutile de stocker ceci dans une variable, utilisez simplement la fonction `display()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codez ici - N'hésitez pas à rajouter des cellules de travail pour rendre le tout plus lisible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 : Nettoyage de données\n",
    "\n",
    "Le nettoyage de données consiste à supprimer/modifier les mauvaises de données, elles peuvent être de plusieurs formes : \n",
    "- Données manquantes\n",
    "- Données dupliquées\n",
    "- Données au mauvais format\n",
    "- Données incorrectes\n",
    "\n",
    "Il est préférable de nettoyer les données pour plusieurs raisons :\n",
    "- Eviter les problèmes de calculs (données manquantes / données aberrantes = résultat incorrect)\n",
    "- Eviter les affichages étranges de graphiques\n",
    "- Uniformiser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemples de code pour nettoyer les données\n",
    "\n",
    "- [Voir documentation de pandas](https://pandas.pydata.org/pandas-docs/stable/reference/index.html)\n",
    "\n",
    "**Permet de supprimer lignes avec des données absentes**\n",
    "\n",
    "Le paramètre \"inplace\" permet de faire muter le dataframe quand le paramètre est égal à \"True\", inplace=False renvoie un nouveau dataframe\n",
    "```python \n",
    "df.dropna(inplace = True)\n",
    "```\n",
    "\n",
    "**Permet de supprimer lignes avec des données non conformes aux conditions définies**\n",
    "\n",
    "(Fonctionne également avec les colonnes)\n",
    "\n",
    "**Renvoie** un nouveau DataFrame.\n",
    "```python \n",
    "df.drop(df[<nos conditions>].index)\n",
    "```\n",
    "\n",
    "**Permet de supprimer lignes avec des données dupliquées**\n",
    "\n",
    "**Ne renvoie pas** un nouveau DataFrame.\n",
    "```python \n",
    "df.drop_duplicates(inplace = True)\n",
    "```\n",
    "\n",
    "**Permet de remplir les cases vides d'une serie, par la valeur moyenne de la serie**\n",
    "\n",
    "La fonction retourne un nouveau dataframe, toutefois, il est possible de faire muter le dataframe en rajoutant le paramètre \"inplace = True\"\n",
    "```python \n",
    "df['nom_de_la_colonne'].fillna(df['nom_de_la_colonne'].mean())\n",
    "```\n",
    "\n",
    "**Permet de changer le type d'une colonne**\n",
    "\n",
    "Ceci peut permettre, notamment de diminuer/augmenter la taille en mémoire d'un dataset (`df.info(memory_usage='deep')`)\n",
    "```python \n",
    "df['nom_de_la_colonne'] = df['nom_de_la_colonne'].astype(nom_du_type)\n",
    "```\n",
    "- [Voir liste des types - Documentation numpy](https://numpy.org/doc/stable/reference/arrays.dtypes.html)\n",
    "\n",
    "**Permet d'appliquer une fonction sur une colonne (ne pas oublier de retourner la valeur)**\n",
    "```python\n",
    "def ma_fonction(val):\n",
    "    return val + 2\n",
    "df['nom_de_la_colonne'] = df['nom_de_la_colonne'].apply(ma_fonction)\n",
    "```\n",
    "\n",
    "Note : si la fonction est un simple calcul, il est possible d'appliquer sur la même ligne le calcul comme le code suivant :\n",
    "```python\n",
    "# Divise toutes les valeurs de la colonne par 2\n",
    "df['nom_de_la_colonne'] = df['nom_de_la_colonne'] / 2\n",
    "\n",
    "# ATTENTION : Il est préférable de s'assurer que sa série est bien numérique avant d'appliquer une opération arithmétique sinon pandas lèvera une erreur à coup sûr.\n",
    "```\n",
    "\n",
    "**Permet de filtrer les lignes par condition (& pour \"et\", | pour \"ou\")**\n",
    "\n",
    "Les parenthèses sont obligatoires.\n",
    "```python\n",
    "df[(df[\"nom_de_la_colonne_1\"] == \"valeur\") | (df[\"nom_de_la_colonne_2\"] == \"valeur\")]\n",
    "```\n",
    "Condition plus complexe \n",
    "```python\n",
    "df[\n",
    "    ((df[\"nom_de_la_colonne_1\"] == \"valeur\") | (df[\"nom_de_la_colonne_2\"] == \"valeur\")) &\n",
    "    (df[\"nom_de_la_colonne_3\"] == \"valeur\")\n",
    "]\n",
    "```\n",
    "\n",
    "**Permet de caster une serie / colonne**\n",
    "\n",
    "```python\n",
    "df['nom_de_la_colonne'] = df['nom_de_la_colonne'].astype('type')\n",
    "```\n",
    "- [Voir liste des types](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dtypes)\n",
    "\n",
    "**Permet de remplacer une valeur dans une colonne**\n",
    "\n",
    "```python\n",
    "df['nom_de_la_colonne'] = df['nom_de_la_colonne'].str.replace('valeur_a_remplacer', 'nouvelle_valeur')\n",
    "```\n",
    "> Note 1 : Cette méthode permet également l'utilisation de regex\n",
    "> Note 2 : Vous aurez besoin de ça pour transformer la colonne \"PRENOM\" dont certains prénoms se sont vus adjoindre \"_1\", \"_2\" ou encore \"_4\" ou encore retirer le virgule (,) dans la colonne \"OBS_VALUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyez vos données ici... mais avant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kévin, Kevin, Elise ou Élise ?\n",
    "\n",
    "Comme expliqué précédemment, le nettoyage des données est une étape importante pour s'assurer de travailler avec des données saines. Dans le cas des prénoms, on peut être face à des problèmes liés aux accents. Si les prénoms \"Kévin\" et \"Kevin\" représentent le même prénom et sont prononcés de la même façon pour un être humain, pour un ordinateur ce n'est pas la même. Ainsi, il est indispensable de substituer toutes les lettres avec accents avec leurs équivalents sans accents. De ce fait, on obtiendra tous les \"Kevin\" (accent ou non) de notre document. Avec Pandas, on le fait avec le code suivant :\n",
    "\n",
    "```python\n",
    "# Le code ci-dessous remplace toutes les prénoms dans la colonne \"preusuel\" avec leur équivalent sans accents\n",
    "df[\"preusuel\"] = df[\"preusuel\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "```\n",
    "\n",
    "Si dans un autre DataFrame, vous avez plusieurs colonnes que vous souhaitez convertir, vous pouvez opérer de la façon suivante :\n",
    "```python\n",
    "list_cols = df.select_dtypes(include=[np.object]).columns\n",
    "df[list_cols] = df[list_cols].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))\n",
    "```\n",
    "\n",
    "Cette logique s'applique également aux majuscules. Sauf cas précis, il est préférable de faire des comparaisons de chaînes de caractères avec la même casse. Avec Pandas, on transformera la casse d'une Serie grâce à la méthode `.str.lower()`. Exemple :\n",
    "\n",
    "```python\n",
    "# On compare la colonne \"key\" qu'on a passé en minuscule avec la valeur \"value\" elle-même passée en minuscules\n",
    "df_result = df[(df[\"key\"].str.lower() != value.lower())]\n",
    "```\n",
    "Note : Il existe également la méthode inverse de `.str.lower()` qui est `.str.upper()`.\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on applique des opérations sur un DataFrame, il est souvent préférable de travailler à partir d'une copie. En effet, durant cette phase de nettoyage nous allons être amené à supprimer des lignes ou encore altérer des valeurs directement sur le DataFrame (paramètre `inplace=true`), mais que ce passe-t-il si nous trompons dans nos opérations ? Nous sommes obligés de compiler toutes les cellules depuis le début, fastidieux et ça prend beaucoup de temps si le jeu de données original est très lourd.\n",
    "\n",
    "De ce fait, durant cette phase de nettoyage, deux options s'offrent à nous :\n",
    "- Ne pas utiliser le paramètre `inplace=true` et mettre les résultats dans des variables différentes\n",
    "- Faire une copie du DataFrame originel grâce à la méthode `.copy()`. Par exemple :\n",
    "```python\n",
    "df_copy = df.copy()\n",
    "# Nettoyage du DataFrame\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liste_prenoms_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Nettoyez vos données ici avec le DataFrame prenoms_nettoyage, nous allons utiliser la technique de la copie\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m prenoms_nettoyage \u001b[38;5;241m=\u001b[39m \u001b[43mliste_prenoms_source\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Pour les années, pour être sûr que toutes les valeurs sont des années, \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# nous allons faire un filtre pour retirer toutes les lignes dont la valeur pour\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# pour la colonne \"annais\" n'est pas numérique. \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# C'est ce que fait la ligne suivante en appliquant un masque sur la colonne \"annais\" :\u001b[39;00m\n\u001b[1;32m      9\u001b[0m prenoms_nettoyage \u001b[38;5;241m=\u001b[39m prenoms_nettoyage[prenoms_nettoyage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannais\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39misnumeric() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'liste_prenoms_source' is not defined"
     ]
    }
   ],
   "source": [
    "# Nettoyez vos données ici avec le DataFrame prenoms_nettoyage, nous allons utiliser la technique de la copie\n",
    "prenoms_nettoyage = liste_prenoms_source.copy()\n",
    "\n",
    "# Pour les années, pour être sûr que toutes les valeurs sont des années, \n",
    "# nous allons faire un filtre pour retirer toutes les lignes dont la valeur pour\n",
    "# pour la colonne \"YOB\" n'est pas numérique. \n",
    "# C'est ce que fait la ligne suivante en appliquant un masque sur la colonne \"YOB\" :\n",
    "\n",
    "prenoms_nettoyage = prenoms_nettoyage[prenoms_nettoyage[\"YOB\"].str.isnumeric() == True]\n",
    "\n",
    "# Toutefois, dépendamment des questions auxquelles vous voulez répondre \n",
    "# le filtre précédent peut être inutile et donc être commenté"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A vous de coder\n",
    "\n",
    "A partir du DataFrame des prénoms **nettoyés**, définir des DataFrame correspondants aux critères suivants (une ligne, un nouveau DataFrame) :\n",
    "\n",
    "1. Contient tous les prénoms uniques du DataFrame et le nombre prénom uniques\n",
    "1. Contient 10 000 premières entrées concernant les naissances d'enfants de sexe féminin dans toute la France\n",
    "   - 1 = garçon | 2 = fille \n",
    "1. Contient toutes les naissances d'enfants ayant votre prénom (ou un autre) dans toute la France\n",
    "1. Contient toutes les naissances de l'année de votre naissance dans le département de votre choix\n",
    "1. Contient toutes les naissances de l'année de votre naissance dans la région de votre choix (plusieurs départements) \n",
    "   - Le DataFrame final doit contenir une colonne contenant le nom de la région\n",
    "1. Contient toutes les naissances de l'année de votre naissance, dix avant et après votre naissance dans toute la France\n",
    "\n",
    "N'oubliez pas que la syntaxe Python présentée dans le document d'initiation à Python fonctionne également avec pandas.\n",
    "\n",
    "**Note :** Pour la question 4, il y a à votre disposition une variable \"data_regions\". C'est un tableau d'objets contenant pour chaque entrée, le numéro, le nom et le nom de la région pour chaque département français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_regions = [{\"number\":\"01\",\"name\":\"Ain\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"02\",\"name\":\"Aisne\",\"region\":\"Hauts-de-France\"},{\"number\":\"03\",\"name\":\"Allier\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"04\",\"name\":\"Alpes-de-Haute-Provence\",\"region\":\"Provence-Alpes-Côte d’Azur\"},{\"number\":\"05\",\"name\":\"Hautes-Alpes\",\"region\":\"Provence-Alpes-Côte d’Azur\"},{\"number\":\"06\",\"name\":\"Alpes-Maritimes\",\"region\":\"Provence-Alpes-Côte d’Azur\"},{\"number\":\"07\",\"name\":\"Ardèche\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"08\",\"name\":\"Ardennes\",\"region\":\"Grand Est\"},{\"number\":\"09\",\"name\":\"Ariège\",\"region\":\"Occitanie\"},{\"number\":\"10\",\"name\":\"Aube\",\"region\":\"Grand Est\"},{\"number\":\"11\",\"name\":\"Aude\",\"region\":\"Occitanie\"},{\"number\":\"12\",\"name\":\"Aveyron\",\"region\":\"Occitanie\"},{\"number\":\"13\",\"name\":\"Bouches-du-Rhône\",\"region\":\"Provence-Alpes-Côte d’Azur\"},{\"number\":\"14\",\"name\":\"Calvados\",\"region\":\"Normandie\"},{\"number\":\"15\",\"name\":\"Cantal\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"16\",\"name\":\"Charente\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"17\",\"name\":\"Charente-Maritime\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"18\",\"name\":\"Cher\",\"region\":\"Centre-Val de Loire\"},{\"number\":\"19\",\"name\":\"Corrèze\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"2A\",\"name\":\"Corse-du-Sud\",\"region\":\"Corse\"},{\"number\":\"2B\",\"name\":\"Haute-Corse\",\"region\":\"Corse\"},{\"number\":\"21\",\"name\":\"Côte-d’Or\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"22\",\"name\":\"Côtes-d’Armor\",\"region\":\"Bretagne\"},{\"number\":\"23\",\"name\":\"Creuse\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"24\",\"name\":\"Dordogne\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"25\",\"name\":\"Doubs\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"26\",\"name\":\"Drôme\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"27\",\"name\":\"Eure\",\"region\":\"Normandie\"},{\"number\":\"28\",\"name\":\"Eure-et-Loir\",\"region\":\"Centre-Val de Loire\"},{\"number\":\"29\",\"name\":\"Finistère\",\"region\":\"Bretagne\"},{\"number\":\"30\",\"name\":\"Gard\",\"region\":\"Occitanie\"},{\"number\":\"31\",\"name\":\"Haute-Garonne\",\"region\":\"Occitanie\"},{\"number\":\"32\",\"name\":\"Gers\",\"region\":\"Occitanie\"},{\"number\":\"33\",\"name\":\"Gironde\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"34\",\"name\":\"Hérault\",\"region\":\"Occitanie\"},{\"number\":\"35\",\"name\":\"Ille-et-Vilaine\",\"region\":\"Bretagne\"},{\"number\":\"36\",\"name\":\"Indre\",\"region\":\"Centre-Val de Loire\"},{\"number\":\"37\",\"name\":\"Indre-et-Loire\",\"region\":\"Centre-Val de Loire\"},{\"number\":\"38\",\"name\":\"Isère\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"39\",\"name\":\"Jura\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"40\",\"name\":\"Landes\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"41\",\"name\":\"Loir-et-Cher\",\"region\":\"Centre-Val de Loire\"},{\"number\":\"42\",\"name\":\"Loire\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"43\",\"name\":\"Haute-Loire\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"44\",\"name\":\"Loire-Atlantique\",\"region\":\"Pays de la Loire\"},{\"number\":\"45\",\"name\":\"Loiret\",\"region\":\"Centre-Val de Loire\"},{\"number\":\"46\",\"name\":\"Lot\",\"region\":\"Occitanie\"},{\"number\":\"47\",\"name\":\"Lot-et-Garonne\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"48\",\"name\":\"Lozère\",\"region\":\"Occitanie\"},{\"number\":\"49\",\"name\":\"Maine-et-Loire\",\"region\":\"Pays de la Loire\"},{\"number\":\"50\",\"name\":\"Manche\",\"region\":\"Normandie\"},{\"number\":\"51\",\"name\":\"Marne\",\"region\":\"Grand Est\"},{\"number\":\"52\",\"name\":\"Haute-Marne\",\"region\":\"Grand Est\"},{\"number\":\"53\",\"name\":\"Mayenne\",\"region\":\"Pays de la Loire\"},{\"number\":\"54\",\"name\":\"Meurthe-et-Moselle\",\"region\":\"Grand Est\"},{\"number\":\"55\",\"name\":\"Meuse\",\"region\":\"Grand Est\"},{\"number\":\"56\",\"name\":\"Morbihan\",\"region\":\"Bretagne\"},{\"number\":\"57\",\"name\":\"Moselle\",\"region\":\"Grand Est\"},{\"number\":\"58\",\"name\":\"Nièvre\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"59\",\"name\":\"Nord\",\"region\":\"Hauts-de-France\"},{\"number\":\"60\",\"name\":\"Oise\",\"region\":\"Hauts-de-France\"},{\"number\":\"61\",\"name\":\"Orne\",\"region\":\"Normandie\"},{\"number\":\"62\",\"name\":\"Pas-de-Calais\",\"region\":\"Hauts-de-France\"},{\"number\":\"63\",\"name\":\"Puy-de-Dôme\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"64\",\"name\":\"Pyrénées-Atlantiques\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"65\",\"name\":\"Hautes-Pyrénées\",\"region\":\"Occitanie\"},{\"number\":\"66\",\"name\":\"Pyrénées-Orientales\",\"region\":\"Occitanie\"},{\"number\":\"67\",\"name\":\"Bas-Rhin\",\"region\":\"Grand Est\"},{\"number\":\"68\",\"name\":\"Haut-Rhin\",\"region\":\"Grand Est\"},{\"number\":\"69\",\"name\":\"Rhône\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"70\",\"name\":\"Haute-Saône\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"71\",\"name\":\"Saône-et-Loire\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"72\",\"name\":\"Sarthe\",\"region\":\"Pays de la Loire\"},{\"number\":\"73\",\"name\":\"Savoie\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"74\",\"name\":\"Haute-Savoie\",\"region\":\"Auvergne-Rhône-Alpes\"},{\"number\":\"75\",\"name\":\"Paris\",\"region\":\"Ile-de-France\"},{\"number\":\"76\",\"name\":\"Seine-Maritime\",\"region\":\"Normandie\"},{\"number\":\"77\",\"name\":\"Seine-et-Marne\",\"region\":\"Ile-de-France\"},{\"number\":\"78\",\"name\":\"Yvelines\",\"region\":\"Ile-de-France\"},{\"number\":\"79\",\"name\":\"Deux-Sèvres\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"80\",\"name\":\"Somme\",\"region\":\"Hauts-de-France\"},{\"number\":\"81\",\"name\":\"Tarn\",\"region\":\"Occitanie\"},{\"number\":\"82\",\"name\":\"Tarn-et-Garonne\",\"region\":\"Occitanie\"},{\"number\":\"83\",\"name\":\"Var\",\"region\":\"Provence-Alpes-Côte d’Azur\"},{\"number\":\"84\",\"name\":\"Vaucluse\",\"region\":\"Provence-Alpes-Côte d’Azur\"},{\"number\":\"85\",\"name\":\"Vendée\",\"region\":\"Pays de la Loire\"},{\"number\":\"86\",\"name\":\"Vienne\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"87\",\"name\":\"Haute-Vienne\",\"region\":\"Nouvelle-Aquitaine\"},{\"number\":\"88\",\"name\":\"Vosges\",\"region\":\"Grand Est\"},{\"number\":\"89\",\"name\":\"Yonne\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"90\",\"name\":\"Territoire de Belfort\",\"region\":\"Bourgogne-Franche-Comté\"},{\"number\":\"91\",\"name\":\"Essonne\",\"region\":\"Ile-de-France\"},{\"number\":\"92\",\"name\":\"Hauts-de-Seine\",\"region\":\"Ile-de-France\"},{\"number\":\"93\",\"name\":\"Seine-Saint-Denis\",\"region\":\"Ile-de-France\"},{\"number\":\"94\",\"name\":\"Val-de-Marne\",\"region\":\"Ile-de-France\"},{\"number\":\"95\",\"name\":\"Val-d’Oise\",\"region\":\"Ile-de-France\"},{\"number\":\"971\",\"name\":\"Guadeloupe\",\"region\":\"Guadeloupe\"},{\"number\":\"972\",\"name\":\"Martinique\",\"region\":\"Martinique\"},{\"number\":\"973\",\"name\":\"Guyane\",\"region\":\"Guyane\"},{\"number\":\"974\",\"name\":\"La Réunion\",\"region\":\"La Réunion\"},{\"number\":\"976\",\"name\":\"Mayotte\",\"region\":\"Mayotte\"}]\n",
    "\n",
    "# Vous pouvez ensuite extraire tous les numéros de département d'une région en particulier de la façon suivante :\n",
    "departments_list_for_region = [p[\"number\"] for p in data_regions if p[\"region\"] == \"Ile-de-France\"]\n",
    "\n",
    "# Et enfin utiliser la méthode \"isin\" entre la colonne de département et le résultat de l'instruction précédente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupons-les - Suite\n",
    "\n",
    "Dans la partie précédente, nous avons vu qu'il était possible de grouper nos DataFrame selon un ou plusieurs colonnes. Ceci étant pratique pour récupérer un aggrégat de certaines colonnes.\n",
    "Toutefois que devons-nous faire lorsque nous souhaitons récupérer les N premiers / derniers lignes de chaque groupe ? Encore une fois un groupe. Prenons la question suivante : \"Quels sont les 5 prénoms les moins populaires des trois années (de votre choix) du sexe féminin ?\". Pour se faire, il faudra procéder de la façon suivante :\n",
    "\n",
    "```python\n",
    "top_n_derniers_prenoms = (\n",
    "    prenoms_nettoyage[prenoms_nettoyage[\"sexe\"] == 2]\n",
    "        .groupby(['prenom', 'annee'], as_index=False)[\"nombre\"] # On groupe par series \"PRENOM\" et \"YOB\"\n",
    "        .sum() # On fait la somme sur la serie \"nombre\"\n",
    ")\n",
    "\n",
    "top_n_derniers_prenoms = (\n",
    "    # On regroupe par année et...\n",
    "    top_n_derniers_prenoms.groupby(['annee'], as_index=False)\n",
    "        # ..on retourne les 5 plus petites valeurs pour la colonne \"nombre\"\n",
    "        .apply(lambda grp: grp.nsmallest(5, \"nombre\"))\n",
    ") \n",
    "\n",
    "# On réinitialise les colonnes ensuite (facultatif)\n",
    "top_n_derniers_prenoms.reset_index(drop=True, inplace=True)\n",
    "```\n",
    "\n",
    "Essayez ce code dans la cellule de code suivante.\n",
    "\n",
    "Le code ci-dessus utilise des parenthèses pour permettre l'écriture d'instructions multilignes, dedans on peut donc faire des retours à la ligne et donc rendre nos instructions plus claires. Par ailleurs, on aurait pu tout faire en une instruction, mais il est toujours préférable de séparer sa logique pour plus de lisibilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testez le code de la cellule précédente ici.\n",
    "# Note : vous êtes bien évidemment invité à modifier le code pour mieux vous approprier le sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En analyse de données, notre but est de se poser des questions (ou recevoir des questions de l'équipe produit) et bien évidemment d'y répondre grâce à la donnée, pour enfin en conclure quelque chose. N'oubliez pas _qu'un problème bien posé est à moitié résolu_. Ce sont des questions qui vont piloter vos DataFrame, votre code, vos articles.\n",
    "\n",
    "# A vous de coder\n",
    "\n",
    "A partir du DataFrame des prénoms, répondre aux questions suivantes avec une variable ou un DataFrame :\n",
    "1. Quel est le prénom masculin et féminin le plus populaire des années 1995 et 2015 dans la France entière (Territoire d'Outre-Mer + Metropole) ?\n",
    "   - Quelle part de naissances représentent ces prénoms sur la totalité des naissances des années (total et par sexe) ? Qu'observez-vous ?\n",
    "2. Pour un prénom choisi, calculer le nombre de naissances par an pour ce prénom\n",
    "   - Il faudra donc grouper par annee et effectuer un somme grâce à la méthode `.sum()` \n",
    "3. Posez-vous une question, essayez d'y répondre avec vos connaissances de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre question, essayez d'y répondre avec vos connaissances de pandas (A REMPLACER par votre question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporter vos DataFrame\n",
    "\n",
    "Avant de terminer, vous avez dû remarquer que consulter vos DataFrame dans Jupyter n'est pas forcément des plus pratiques notamment à cause de la troncature du rendu final. Heureusement pour vous, pandas permet d'exporter vos DataFrame sous plusieurs formats via des méthodes dédiées : .csv (to_csv), .xslx (to_excel) ou encore .sql (to_sql). \n",
    "```python\n",
    "# Ceci nous permet de créer un csv nommé \"mon-csv.csv\"\n",
    "df.to_csv(\"mon-csv.csv\")\n",
    "```\n",
    "\n",
    "A noter que si le fichier que vous êtes en train de créer est ouvert dans un autre logiciel, l'écriture du fichier risque d'échouer.\n",
    "- [Voir documentation de la méthode \"to_csv\"](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html?highlight=to_csv#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
